# Awesome-Diffusion-MoE
[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/wangqiang9/Awesome-Diffusion-MoE) 
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)

## Table of Contents
- [Token Choice]()
- [Expert Choice]()


## Token Choice

HunyuanImage-3.0: A Powerful Native Multimodal Model for Image Generation

[ğŸ“„ Paper](https://arxiv.org/pdf/2509.23951) | [ğŸŒ Project Page](https://github.com/Tencent-Hunyuan/HunyuanImage-3.0) | [ğŸ’» Code](https://github.com/Tencent-Hunyuan/HunyuanImage-3.0)

Routing Matters in MoE: Scaling Diffusion Transformers with Explicit Routing Guidance

[ğŸ“„ Paper](https://arxiv.org/abs/2510.24711)

Dense2MoE: Restructuring Diffusion Transformer to MoE for Efficient Text-to-Image Generation

[ğŸ“„ Paper](https://arxiv.org/abs/2510.09094)

Diff-MoE: Diffusion Transformer with Time-Aware and Space-Adaptive Experts

[ğŸ“„ Paper](https://openreview.net/pdf?id=JCUsWrwkKw) | [ğŸ’» Code](https://github.com/kunncheng/Diff-MoE)

DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers

[ğŸ“„ Paper](https://arxiv.org/abs/2503.14487) | [ğŸŒ Project Page](https://shiml20.github.io/DiffMoE/) | [ğŸ’» Code](https://github.com/KwaiVGI/DiffMoE.git)

Efficient Training of Diffusion Mixture-of-Experts Models: A Practical Recipe

[ğŸ“„ Paper](https://arxiv.org/abs/2512.01252)

Scaling Diffusion Transformers to 16 Billion Parameters

[ğŸ“„ Paper](https://arxiv.org/abs/2407.11633) | [ğŸ’» Code](https://github.com/feizc/DiT-MoE)


## Expert Choice

Expert Race: A Flexible Routing Strategy for Scaling Diffusion Transformer with Mixture of Experts

[ğŸ“„ Paper](https://arxiv.org/abs/2503.16057)

EC-DIT: Scaling Diffusion Transformers with Adaptive Expert-Choice Routing

[ğŸ“„ Paper](https://arxiv.org/abs/2410.02098)



